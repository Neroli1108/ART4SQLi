% This is "sig-alternate.tex" V2.0 May 2012
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{psfrag}
\usepackage{psfig}
\usepackage{cite}
\usepackage{enumerate}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{algorithm} 
\usepackage{algorithmic} 
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{xcolor}
%\DeclareMathOperator*{argmin}{argmin} 
%\enewcommand{algorithmicrequire}{ extbf{Input:}} 
%\enewcommand{algorithmicensure}{ extbf{Output:}} 
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{WOODSTOCK}{'97 El Paso, Texas USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{The ART of SQL Injection Vulnerabilities Discovery \titlenote{This research was supported by the Open Fund of the State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences (Grant No. SYSKF1405).}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{6} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
% 
% 1st. author
\alignauthor
Chenghong Wang\\
%\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Harbin Engineering University}\\
%       \affaddr{144 Nantong Street}\\
       \affaddr{Harbin, China}\\
       \email{wangchenghong@hr-beu.edu.cn}
% 2nd. author
\alignauthor
Donghong Zhang\\
       \affaddr{Harbin Engineering University}\\
%       \affaddr{144 Nantong Street}\\
       \affaddr{Harbin, China}\\
       \email{zhangdonghong@hr-beu.edu.cn}
% 3rd. author
\alignauthor Jing Zhao\titlenote{All correspondence should be forwarded to Dr. Jing Zhao}\\
       \affaddr{Harbin Engineering University}\\
%       \affaddr{144 Nantong Street}\\
       \affaddr{Harbin, China}\\
       \email{zhaoj@hrbeu.edu.cn} 
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Jingyu Li\\
       \affaddr{Harbin Engineering University}\\
       \affaddr{Harbin, China}\\
       \email{lijingyu@hrbeu.edu.cn} 
% 5th. author
\alignauthor Hualin Lu\\
       \affaddr{Harbin Engineering University}\\
       %       \affaddr{144 Nantong Street}\\
       \affaddr{Harbin, China}\\
       \email{luhualin@hrbeu.edu.cn}
% 6th. author
\alignauthor Long Zhang\\
       \affaddr{Institute of Software}\\
       \affaddr{Chinese Academy of Sciences}\\
       \affaddr{Beijing 100190}\\
       \email{zlong@ios.ac.cn}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Zhenyu Zhang\\
       \affaddr{Institute of Software}\\
       \affaddr{Chinese Academy of Sciences}\\
       \affaddr{Beijing 100190}\\
       \email{zhangzy@ios.ac.cn} 
\alignauthor Jian Zhang\\
       \affaddr{Institute of Software}\\
       \affaddr{Chinese Academy of Sciences}\\
       \affaddr{Beijing 100190}\\
       \email{zj@ios.ac.cn}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
%\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
%email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
%(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
%\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
SQL injection(SQLi) is one of the  top risky vulnerabilities for web applications which may lead to significant security consequences such as authentication bypassing, privacy leakage, etc.. Therefore, discovering SQLi vulnerabilities is an important step for ensuring its quality. Effective testing techniques can ensure that applications are free from SQLi vulnerabilities before its been released and reduce related cost of performing manual analysis, monitoring or post deployment of other defensive mechanisms. However, due to the variety of SQLi attack types the generated test cases for discovering SQLi are scable and complex. Moreover, successful test cases which could discovery SQLi vulnerabilities consist only a small part of the whole test case space. Therefore, testing the SQLi vulnerabilities is a time consuming and inefficient task. Towards this problem we propose an effective SQLi testing approch based on Adaptive Random Testing(ART) method. We define the concept of test case distance and then present the computing method for test case distance by using TF-IDF algorithm. Then we apply the Fixed Size Candidate Set(FSCS) algorithm in SQLi test case selection with the help of test case distance metrics. We also conduct 6 experimental studies on our testing algorithm, where we tested the SQLi vulnerabilities in 4 open source benchmarks and 2 CMS applications. The experimental results indicate that our testing algorithm can reduce the redundant test cases by more than 50\% and significantly increases the testing effectiveness of SQLi vulnerabilities discovering techniques.
\end{abstract}
%A category including the fourth, optional field follows...
\category{D.2.5}{Software Engineering}{Testing and Debugging}
\terms{Security, Algorithms}

\keywords{SQL injection; adaptive random test; test case distance metrics; selection algorithm}

\section{Introduction}
Nowadays, database-driven web applications have been rapidly adopted in a wide range of areas including online stores, e-commerce, social network services, etc. However such popularity makes them more attractive to attackers. The number of reported web attacks is growing sharply\cite{web_security:report}. A recent web application attack report show us that, over a period of nine months, from August 1, 2013 to April 30, 2014 reveals an average increase of around 17\% in different types of web attacks. In addition, the report also found that web attacks have became more sophisticated and gotten dramatically longer in length.(44\% longer than they were in previous reports). Web applications are suffering more than 26 attacks in one minute\cite{web_security:report2}. Other security report indicate that at least 8\% of the web services which belong to some technology companies like Microsoft and Google contains different types of security vulnerabilities\cite{dsn09:scanner}.

Within the Web based vulnerabilities, SQLi vulnerabilities has been labeled as top risky vulnerabilities by {Open Web Application Security\footnote{http://www.programmableweb.com,}}. SQLi refers to a class of code-injection attacks in which data provided by the user is included in a SQL query in such a way that part of the user's input is treated as SQL code\cite{ase05:amneisa}. This type of vulnerability is ultimately caused by insufficient validation of user-input\cite{ase05:amneisa, halfond07:detection, halfond06:classification}. By exploiting such vulnerabilities, attackers can submit malicious requests which contains sophisticated SQL queries to remote database-driven web applications. As a result, attackers will capture the privilege of accessing the underlying database arbitrarily. Attackers can even take control of and corrupt the system that hosts the Web applications by elaborately designing the injected SQL codes. During the past few years, SQLi vulnerabilities have already became the top 1 threat for database-driven web applications, and SQL injection attacks have also become the most popular web attack. 

Although the cause for SQLi is quite simple and the mechanisms of SQLi are well-understood, the SQLi vulnerabilities still increase. They persist because of a lack of effective techniques for discovering and preventing them. Previous approaches to detecting SQLi  vulnerabilities and preventing exploits include static analysis\cite{su06essence, livshits05finding, fu2007static}, user input filtering\cite{leblanc2002writing}, defensive programming\cite{cook2005safe}, runtime monitoring\cite{buehrer2005using, halfond2006using, pietraszek2006defending, su2006essence}, combining static analysis and runtime monitoring\cite{ase05:amneisa}, source code fixing\cite{thomas2007using}, application level intrusion detecting\cite{lin2007automatic}, SQL key words randomization\cite{boyd2004sqlrand}, testing techniques\cite{shahriar2008music, appelt2014automated, kieyzun2009automatic}, etc.. Among these previous researches, it is well accepted that security testing method is the most effective way towards discovering SQLi vulnerabilities. Security testing against SQLi vulnerabilities could significantly reduce the related costs of static analysis, monitoring or deploy application level defensive mechanisms, which are acknowledged time consuming and inefficient tasks. On the other hand, security test could guarantee the web applications free from SQLi vulnerabilities before they go into services. It is much easier to repair the applications before the applications' service lifecycle than during such period. However, very few works address the issues of testing techniques towards SQLi vulnerabilities, as SQLi testing suffers from three well-known problems. The problems for SQLi testing could be conclude as follows:
\begin{enumerate}[a)]
\item \emph{Scalability of Test Case Space}

The difficulty for researchers to address the full scope of the SQLi vulnerabilities is that there are many types of SQLI attacks and countless variations on these basic types. As a result, the test case space should be designed as large as possible. Otherwise some effective attack vectors which could reveal the potential SQLi vulnerabilities of testing applications may be missed.( i.e. Functions and keywords filtering prevents web applications from being attacked by using a functions and keywords black list. If an attackers submits an injection code containing a keyword or SQL function in the black list, the injection will be unsuccessful. Although the technique is very strict and perform in real world, applications with such security mechanism may also vulnerable to SQLi attacks. Attacks can evade inspection by using case changing, character encoding or inline comment methods.) Therefore, it is important to design enough test cases for testing. Equally the scale of test case space increase to a great level.
\item \emph{Sparse Distribution of Effective Test Cases}

Though the test case space is very large, the effective test cases which could flag SQLi vulnerabilities distributed sparsely. In our previous works, we found that the effective test cases for SQLi discovery consist only 2\% of the total test case pool. Such small proportion makes the searching task for effective test cases more difficult than normal testing tasks. And the performing of rest 98\% can be regard as redundant works which may limit the testing effectiveness. 
\item \emph{Large Expense for Executing SQLi Test Cases}

The performing costs for SQLi vulnerabilities discovery are also very high. The judgement of test cases execution results could be a complex work. Some researchers have proposed some automatic mechanism for testing result checking and judging\cite{appelt2014automated, damele2012sqlmap, riancho2011w3af}. However, in some cases, it also requires some professional testers to complete the checking and judging tasks manually. i.e. In some web applications the injectable parameters appear in some particular sections like HTTP Head Referer (CVE-2011-3340\footnote{http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2011-3340}, CVE-2008-0850\footnote{http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2008-0850}, CVE-2007-1061\footnote{http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2007-1061}) section. In this cases, the most accurate and effective way to check or judge the testing is to check by testing engineers. Testing engineer have to check every test cases results during the testing period. 
\end{enumerate}

According to all the aforementioned problems of SQLi testing techniques, we know that SQLi testing is indeed a timing consuming and complex task compared to normal software testing. Moreover the high level of testing redundancy for SQLi vulnerabilities discovering even amplify the testing ineffectiveness and complexity. How to reduce the verbosity of test cases have became an important problem for improving SQLi testing effectiveness. To address this problem, we propose a test case selection algorithm for SQLi vulnerabilities discovering in this paper based on Adaptive Random Testing(ART) method. Through various empirical observations showing that many successful test cases are very similar between each others (i.e. Some are encoded by the same encoding method or attached with similar prefix.). More specifically, the successful test cases which could discovery SQLi have a high degree of similarity. As a result successful test cases distributed together to form clusters in total test cases domain. 

Due to this distribution of successful test cases, ART is supposed to be the best choice to improve the effectiveness of test case selection tasks. In our work, the approach started from test cases distance metrics quantification, in this part we apply the TF-IDF algorithm to compute the test cases distance due to the previous observation consequences of the successful SQLi test cases distribution. Then we apply a well performed Adaptive Random Testing method called Fixed Size Candidate Set(FSCS) method to select test cases with the help of test cases distance metrics. Through FSCS algorithm, the test case selected to execute should be the most distinct (the farthest) one compared to the previous one. It is because that regions of the unsuccessful test cases domain will also be contiguous. Thus, given a set of previously executed test cases that have not revealed any failures, new test cases located away from these old ones are more likely to reveal failures. 

We have also evaluated our test cases selection algorithm by comparing the discovering effectiveness of some reported vulnerabilities(reported on National Vulnerability Database) between testing approach with and without FSCS-ART algorithm. The statistic used to compare the methods was the average number of tests required to detect the first successful test case, which is commonly known as the F-measure\cite{chen2005adaptive}. In most cases, the F-measure of FSCS-ART was 30-50\% lower than that of testing approach without ART selection algorithm. Experimental result indicate that our selection can significantly reduce the SQLi testing redundancy and increase the SQLi vulnerabilities discovering effectiveness.

The main contributions of this work are:
\begin{itemize}
\item The presentation of a new technique to test SQLi vulnerabilities that based on adaptive random method.
\item The observation and analysis of effective test case distribution for dectecting SQLi vulnerabilities which shows that successful test case tend to be clustered together. 
\item The presentation of test case distance metrics based on string similarity metrics.
\item An experimental evaluation of the technique that shows the effectiveness
and the efficiency of this technique.
\end{itemize}

The remainder of this paper is organised as follows: Section 2 and Section 3 provides a background on SQLi vulnerabilities and reviews related work. Section 4 is the introduction of Adaptive Random Testing and discussion of successful SQLi test case distribution. Section 5 presents our proposed security testing approach. Section 6 presents the evaluation together with a discussion of results and threats to validity. Finally, Section 7 concludes the work.



\section{Background of SQLi}
\subsection{Injection Mechanisms}
\subsection{Example of SQL injections}
\section{Related Works}
\subsection{SQLi Prevention}
\subsection{Adaptive Random Testing}
Adaptive Random Testing (ART) is an effective improvement of Random Testing (RT). It is based on the observation effective test cases for testing target program tend to be clustered together. ART, therefore, proposes to have randomly selected test cases being more evenly spread throughout the test case domain by employing the location information of the successful test cases.\cite{39239038}

\section{Test Case Distributions}
Essentially, the testing process can be viewed as taking samples
from the set of all SQLi attack payloads(known as the test case space) to the web application under test, executing the samples one by one, and determining whether the malicious inouts been filtered by input validation mechanisms of testing application. If the inputs get through, a payload bypassing is revealed. The presence of a bypassing implies the existence of a SQLi vulnerability. A tester seeks to select test data 
with a view to minimising the cost for detecting SQLi vulnerabilities. 
To help the tester in this task, it is natural to firstly consider how
effective test cases distribute in total test cases space.

To present our work clearly, let us first define a few terms. Suppose $\mathit{T}$ = \{$t_{1}$, $t_{2}$, $t_{3}$, ... , $t_{n}$ \} is a SQLi test case space for some specific web application program (denoted by $P$) with $n$ test cases. For each test case $t_{i}$ in $T$ is a type of specific string which could be injeted into web applications through input functions and flag SQLi attacks, we also call these strings SQLi attack payloads. Within test case space $T$ we define $S$ = \{$s_{1}$, $s_{2}$, $s_{3}$, ... , $s_{k}$ \} to be the effective test cases set with total $k$ test cases, where every test case $s_{i}$ in $S$ can successfully flag SQLi vulnerabilities in application $P$. On the other hand $F$ = \{$f_{1}$, $f_{2}$, $f_{3}$, ... , $f_{d}$ \} are defined as ineffective test cases set or redundant test cases set such that $ F, S\subseteq T$ and $F \cap S = \varnothing$.
The size number of total test case space, effective test case space and redundant test case space are denoted by $n, k, d$ respectively, where $n = k + d$.

In our recent works, we have tested 29 reported SQLi vulnerabilities (Reported on NVD) by using the test cases (SQLi attack payloads) from real world. After that we have two observations from our testing. First, the test cases space collected for dectecting SQLi vulnerabilities is very huge, in other word the $n$ is very large. However, the effective test cases consist only a small part of the test cases space, the successful ratio $k / n$ is always below 2 percent. Second, effective SQLi test cases which could detect some specific SQLi vulnerability are always have the similar string format or constructure. 

Let's take the CVE-2010-0425~(a reported SQLi vulnerability) as an example. Table I is the effective test cases we collected in our testing, it shows that the effective test cases for detecting CVE-2010-0425 are all similar to each others. All successful test cases are tautology attack payloads and the injection type are all the time based blind SQL injection mode (they contain the same terms like waitfor, delay, which are the key words for time based blind SQLi). To improve that the effective test cases share high similarity between each others, we select one effective test case $s_{j}$ and then compute the string similarity between $s_{j}$ and other test cases used for dectecting CVE-2010-0425.

\begin{table}
\centering
\caption{Successful Test Cases for CVE2010-0415}
\begin{tabular}{|c|c|l|} \hline
No.&Sucessful Test Case\\ \hline
1 & ; or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
2 & ); or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
3 & '; or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
4 & "; or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
5 & '); or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
6 & "); or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
7 & )); or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
8 & ')); or 1 = 1 waitfor delay '0: 0: TIME --\\ \hline
... & ...\\
\hline\end{tabular}
\end{table}

From the details we observed, we can conclude that, effective SQLi test cases for testing target tend to be clustered together within whole test case domain, in other word if $s_{j}$ is an effective test cases twords some specific SQLi vulnerability, the test cases close to $s_{j}$ (similar with $s_{j}$ in string format) will more likely to reveal such vulnerability.

\section{The ART of SQLi Discovery}
Clustering distribution of effective test cases for detecting SQLi are indeed common. Therefore, given a set of previously executed
test cases that have not revealed SQLi vulnerabilities, new test cases
located away from(string format distinct from) these old ones are more likely to reveal vulnerabilities-in other words, test cases should be more evenly spread
throughout the total test case space. According to this intuition, Adaptive
Random Testing (ART) method was applied to improve the effectiveness of SQLi discovering technique.
\subsection{Test Case Similarity Metrics}
Adaptive random testing require that in each round of testing when we generate a new
test case, we need to make sure that the new test case should not be too close
to any of the previously generated ones. Therefore, it is necessary to define the distance metrics first. Because we observe such clustering phenomenon from the perspective of string structure. Thus, we define test case distance based on string similarity, in this paper we apply TFC method and Cosine Similarity in defining test case distance metrics:\\
%From our previous observation of effective SQLi test cases, we found that they, effective SQLi test cases, tend to be clustered together from the string structure point of view. Thus, we define the test case distance metrics based on the string similarity and propose TF-ITSF(Term Frequency-Inverse Testcase Space Frequency) method to quantify test case similarity.\\ 

{\bf Definition 1 [Test Case Term Space]}~Each SQLi test case is a type of string, the elements consist of such string are called test cases terms. Suppose $T$ is the test case space for application $P$. Then, $TS = \{tm_{1},tm_{2},...,tm_{t}\}$ is the test case term space of $T$, denote the set of all distinct terms appear in whole test case space. The element terms in SQLi test cases area not the same terms in documents, they are more complicated. Table 2 provides a summary of the terms in SQLi test cases.\\








{\bf Definition 2 [Test Case Feature Vector]}~Suppose $T = \{t_{1}, t_{2}, t_{3},...,  t_{n}\}$ is the test case space of testing application $P$, and $TS = \{tm_{1}, tm_{2}, tm_{3},..., tm_{k}\}$ is the term space, denote $k$ distinct terms in total test case space. And we use following method to vectorize each test case $t_{j}$:\\

 The vector of $t_{j}$ is denoted as  ${\bf  t_{j}} = [{w_{1,j}, w_{2,j}, w_{3,j} ... , w_{k,j}]}$. Each dimension is called the (term) weight corresponds to a separate term in $TS$, where 

\begin{equation}w_{i,j} = \frac { \log{(TF_{i,j} + 1.0)} \times \log \frac{|T|}{|\{t^{'} : tm_i \in t^{'}\}|} }{\sqrt{ \sum_{i=1}^{k} \lbrack \log{(TF_{i,j} + 1.0)} \times \log \frac{|T|}{|\{t^{'} : tm_i \in t^{'}\}|}\rbrack ^{2} }}
\end{equation}


\begin{itemize}
\item $TF_{i,j}$ is the term frequency of term $tm_{i}$ in test case $t_{j}$.
\item $|T|$ is total number of test cases , and ${|\{t^{'} : tm_i \in t^{'}\}|}$, is number of test cases containing term $tm_{i}$.
\end{itemize} 

Usually, ${\bf  t_{j}}$ is a column vector with high level dimensions, thus such feature vector will bring lot of costs in computing procedures. Here we apply Principal Component Analysis (PCA) method to reduce ${\bf  t_{j}}$.

Suppose ${\bf T}$ is the vectorized representation of test case space. The test case space matrix ${\bf T}$ can be described as below:

$$
{\bf T}=\left({\bf t_{1}},{\bf t_{2}},{\bf t_{3}}, ...,{\bf t_{n}} \right)\\
$$
$$
   {\bf T}=\left(
      \begin{array}{cccccc}
        w_{11} & w_{12} & w_{13} & \cdots & w_{1n} \\ 
        w_{21} & w_{22} & w_{23} & \cdots & w_{2n} \\
        w_{31} & w_{32} & w_{33} & \cdots & w_{3n} \\
         \vdots & \vdots & \vdots & \ddots & \vdots\\
       w_{m1} & w_{m2} & w_{m3} & ... & w_{mn}\\
      \end{array}
    \right)
$$\\
where each colum of matrix ${\bf T}$ corresponds to the vector of each test case Before we apply principle component analysis method, we need to firstly standardized the weight data in matrix $T$, here we use $z-score$[**] to standardize the term weight data. Suppose ${\bf T'}$ is the standardized matrix, ${\bf T'}$ can be represented as follow:
$$
{\bf T'}=\left(
      \begin{array}{cccccc}
        w_{11}' & w_{12}' & w_{13}' & \cdots & w_{1n}' \\ 
        w_{21}' & w_{22}' & w_{23}' & \cdots & w_{2n}' \\
        w_{31}' & w_{32}' & w_{33}' & \cdots & w_{3n}' \\
         \vdots & \vdots & \vdots & \ddots & \vdots\\
       w_{m1}' & w_{m2}' & w_{m3}' & ... & w_{mn}'\\
      \end{array}
    \right)
$$
where the standardized weight $w_{ij}'$ in matrix ${\bf T'}$ can be computed by the following equations:
\begin{equation}
w_{ij}' = \frac{w_{ij} - {\bar w_{i}}}{s_{i}}
\end{equation}
\begin{equation}
{\bar w_{j}} = \frac{\sum_{j=1}^{n}w_{ij}}{n},~s_{j}^2 =\frac{ \sum_{j=1}^{n}(w_{ij}-{\bar x_{i}})^2}{n-1}
\end{equation}
\begin{eqnarray}
\frac{1}{m}\sum_{j=1}^{n}({\bf t_{j}^{\mathrm T}}{e_{i}})^2 = \frac{1}{m}\sum_{j=1}^{n}{e_{i}^{\mathrm T}}{\bf t_{j}}{\bf t_{j}^{\mathrm T}}{e_{i}}\\
=\frac{1}{m}{e_{i}^{\mathrm T}}\left({\sum_{j=1}^{n}{\bf t_{j}}{\bf t_{j}^{\mathrm T}}}\right){e_{i}}\\
={e_{i}^{\mathrm T}}\Sigma{e_{i}}
\end{eqnarray}


{\bf Definition 3 [Test Case Distance Metrics]}~Suppose two test case $t_{i}$ and $t_{q}$, their feature vectors are\\ ${\bf t_{j}} = [w_{1,j}, w_{2,j}, w_{3,j},...,w_{k,j}]^{\mathrm T}$, ${\bf t_{q}} = [w_{1,q}, w_{2,q}, w_{3,q},...,w_{k,q}]^{\mathrm T}$. We define the distance between test case $t_{i}$ and $t_{q}$ by using Cosine Distance( or Cosine Similarity ) measurement. Such distance is represented as follows:

\begin{equation}dis(t_{j}, t_{q}) = \frac{  {\bf t_{j}} \cdot {\bf t_{q} } }{ || {\bf t_{j} } || ~||  {\bf t_{q}} || } = \frac{ \sum_{i=1}^{k} w_{i,j}\cdot w_{i,q} }{\sqrt{  \sum_{i=1}^{k} w_{i,j}^{2} } \cdot \sqrt{ \sum_{i=1}^{k} w_{i,q}^{2}  }  }\end{equation}

In definition 2 , it is obviously that $\forall ~t_{i}~ in ~T$ , the (term) weight in feature vector $\bf d_{i}$ are all belongs to $\mathbb{R}$. Therefore, the outcome of Equation (2), test cases distance (similarity), is neatly bounded in $\lbrack ~0, 1 ~\rbrack$. The resulting distance (similarity) ranges from 0 indicating orthogonality (decorrelation), to 1 meaning exactly the same, and in-between values indicating intermediate similarity.




\subsection{Candidate Test Case Selection Algorithm}
In this paper,  we use the first ART method proposed, Fixed Size Candidate Set (FSCS)[**] algorithm to select candidate test cases. In our process, the selection algorithm makes use of two sets of test cases, named executed set and candidate set which. Suppose $E = \{e_{1}, e_{2}, e_{3}, ..., e_{f}\}$ is the executed set, and $C= \{c_{1}, c_{2}, c_{3}, ..., c_{\kappa}\}$ be the candidate set such that $E \cap C = \emptyset$.  


 $E$ is the set of distinct test cases that have already been selected and executed but without revealing any SQLi vulnerability while the $C$ is a set of test cases that are randomly selected from $T$. The executed set is initially empty and the first test case is randomly chosen from the test case space. The executed set is then incrementally updated with the selected element $c_{h}$, ( $c_{h} \in C$ ) until a SQLi vulnerability is revealed. The selected element $c_{h}$ should satisfy the following condition:
For all $ j \in \{1, 2, 3, ..., \kappa\} $, $$\min_{i=1}^{f} dis(c_{h}, e_{i}) \geq \min_{i=1}^{f} dis(c_{j}, e_{i}) $$
where $dis$ is defined in Definition 3. This selection criterion of FSCS-ART is referred to as $S_{distance}$ in this paper. Specific algorithm of $S_{distance}$ is described in Algorithm \ref{alg:Framwork}.

\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm
\begin{algorithm}[htb] 
\caption{ ~$S_{distance}$ Selection Criterion  } 
\label{alg:Framwork} 
\begin{algorithmic}[1] %??1 ??????????
\REQUIRE ~~\\ %????????Input
The Executed Test Case Set, $E$;\\
Candidate Test Case Set, $C$.

\ENSURE ~~\\ %??????Output
Candidate Test Case, $candidate$.\bigskip\\
%\STATE $E$ = \{\}
%\STATE randomly generate candidate set $C$ from $T\cap\bar{E}$
 %with $\kappa$ candidate test cases $c_{1}, c_{2}, c_{3}, ..., c_{\kappa}$
\label{ code:fram:gen }
\STATE $BestDis = 0$
\FOR {each candidate $c_{j}~in~C$}
\STATE compute $d_{j}$\\where $ d_{j} = \min_{i=1}^{f} dis(c_{j}, e_{i})$
\IF {$d_{j} \geq D$}
\STATE $BestDis~=~d_{j}$
\STATE $candidate = c_{j}$
\ENDIF
\ENDFOR
\RETURN $candidate$; %??????
\end{algorithmic}
\end{algorithm}

Chen et al.\cite{39239038} have observed that the effectiveness of FSCS-ART can be significantly improved by increasing $\kappa$ when $\kappa\geq10$.And he also suggested that $\kappa = 10$ is close to the optimal setting of
FSCS-ART. Hence, they have used $\kappa = 10$ in their investigation.\cite{chen2009adaptive} In our $S_{distance}$ algorithm, we also use the suggested value of $\kappa$, which is 10.

\begin{algorithm}[htb] 
\caption{ Fixed Size Candidate Selection  Algorithm  } 
\label{alg:fscs} 
\begin{algorithmic}[1] 
\REQUIRE ~~\\ 
The set of test case space for application $P$, $T$;\\
The Size of Candidate Set, $\kappa$.\bigskip\\
%\ENSURE ~~\\ %??????Output
%Candidate Test Case, $candidate$.
%\STATE $E$ = \{\}
%\STATE randomly generate candidate set $C$ from $T\cap\bar{E}$
 %with $\kappa$ candidate test cases $c_{1}, c_{2}, c_{3}, ..., c_{\kappa}$
\STATE $E = \{\}$
\STATE randomly select a test case $t_{initial}$ from $T$
\STATE Add $t_{initial}$ in to $E$
\STATE Testing application $P$ by using $t_{initial}$
\IF {$Any~SQLi~vulnerability~is~revealed$}
\STATE STATUS = \TRUE
\ELSE
\STATE STATUS = \FALSE
\ENDIF
\WHILE {\NOT STATUS}
\STATE $C~=~\{\}$
\STATE Randomly select $\kappa$ test cases $c_{1}, c_{2}, c_{3},..., c_{\kappa}$ from $T$
\STATE Add $c_{1}, c_{2}, c_{3},..., c_{\kappa}$ into $C$
\STATE Executing $S_{distance}$ to select candidate test case $c_{h}$.
\STATE Testing application $P$ by using $c_{h}$
\IF {$Any~SQLi~vulnerability~is~revealed$}
\STATE STATUS = \TRUE
\ELSE
\STATE $E~=~E+\{c_{h}\}$
\ENDIF
\ENDWHILE
%\RETURN $candidate$; %??????
\end{algorithmic}
\end{algorithm}

\section{Experiments}
\subsection{Experiment Setup}
\subsection{Scenarios}
\subsection{Result \& Analysis}

%\section{Related Works}


\section{Conlusion}

\bibliographystyle{abbrv}
\bibliography{sqli} 

\appendix
%Appendix A
\section{Headings in Appendices}
The rules about hierarchical headings discussed above for
the body of the article are different in the appendices.
In the \textbf{appendix} environment, the command
\textbf{section} is used to
indicate the start of each Appendix, with alphabetic order
designation (i.e. the first is A, the second B, etc.) and
a title (if you include one).  So, if you need
hierarchical structure
\textit{within} an Appendix, start with \textbf{subsection} as the
highest level. Here is an outline of the body of this
document in Appendix-appropriate form:
\subsection{Introduction}
\subsection{The Body of the Paper}
\subsubsection{Type Changes and  Special Characters}
\subsubsection{Math Equations}
\paragraph{Inline (In-text) Equations}
\paragraph{Display Equations}
\subsubsection{Citations}
\subsubsection{Tables}
\subsubsection{Figures}
\subsubsection{Theorem-like Constructs}
\subsubsection*{A Caveat for the \TeX\ Expert}
\subsection{Conclusions}
\subsection{Acknowledgments}
\subsection{Additional Authors}
This section is inserted by \LaTeX; you do not insert it.
You just add the names and information in the
\texttt{{\char'134}additionalauthors} command at the start
of the document.
\subsection{References}
Generated by bibtex from your ~.bib file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the ~.bbl file.  Insert that ~.bbl file into
the .tex source file and comment out
the command \texttt{{\char'134}thebibliography}.
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
\section{More Help for the Hardy}
The sig-alternate.cls file itself is chock-full of succinct
and helpful comments.  If you consider yourself a moderately
experienced to expert user of \LaTeX, you may find reading
it useful but please remember not to change it.
%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
